{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data (after segmentation)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('1.data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate data and targets\n",
    "dataColumns=len(dataset.columns)\n",
    "X=dataset.iloc[:,0:dataColumns-1].values\n",
    "y=dataset.iloc[:,dataColumns-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29135, 300)\n",
      "(29135,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16709333 -0.16459333 -0.16359333 ...  0.00965833  0.00665833\n",
      "   0.00415833]\n",
      " [ 0.          0.          0.         ... -0.07235    -0.07035\n",
      "  -0.06985   ]\n",
      " [-0.04135    -0.04285    -0.04585    ... -0.01767    -0.02017\n",
      "  -0.02167   ]\n",
      " [ 0.01183     0.00733     0.00433    ...  0.00448167  0.00048167\n",
      "  -0.00401833]\n",
      " [ 0.02848167  0.02098167  0.01498167 ...  0.088015    0.084515\n",
      "   0.079515  ]]\n",
      "['V' '/' '/' '/' '/']\n"
     ]
    }
   ],
   "source": [
    "#View data in numbers\n",
    "print(X[0:5])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19b971ad308>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking 5000 datasamples for each target class\n",
    "numDataPerClass=5000\n",
    "\n",
    "data1=np.zeros((numDataPerClass,dataColumns-1),dtype=float)\n",
    "target1=np.zeros((numDataPerClass,1),dtype=float)\n",
    "\n",
    "data2=np.zeros((numDataPerClass,dataColumns-1),dtype=float)\n",
    "target2=np.zeros((numDataPerClass,1),dtype=float)\n",
    "\n",
    "data3=np.zeros((numDataPerClass,dataColumns-1),dtype=float)\n",
    "target3=np.zeros((numDataPerClass,1),dtype=float)\n",
    "\n",
    "data5=np.zeros((numDataPerClass,dataColumns-1),dtype=float)\n",
    "target5=np.zeros((numDataPerClass,1),dtype=float)\n",
    "\n",
    "data12=np.zeros((numDataPerClass,dataColumns-1),dtype=float)\n",
    "target12=np.zeros((numDataPerClass,1),dtype=float)\n",
    "\n",
    "i=0;\n",
    "count1=0\n",
    "count2=0\n",
    "count3=0\n",
    "count5=0\n",
    "count12=0\n",
    "\n",
    "while ((count1+count2+count3+count5+count12)<numDataPerClass*5):\n",
    "    if(y[i]=='N' and count1<numDataPerClass):#Normal Data\n",
    "        data1[count1,:]=X[i,:]\n",
    "        target1[count1,:]=0\n",
    "        count1=count1+1\n",
    "    elif(y[i]=='/' and count2<numDataPerClass):#Paced beats\n",
    "        data2[count2,:]=X[i,:]\n",
    "        target2[count2,:]=1\n",
    "        count2=count2+1\n",
    "    elif(y[i]=='R' and count3<numDataPerClass):#Right Bundle Branch Block\n",
    "        data3[count3,:]=X[i,:]\n",
    "        target3[count3,:]=2\n",
    "        count3=count3+1\n",
    "    elif(y[i]=='L' and count5<numDataPerClass):#Left Bundle Branch Block\n",
    "        data5[count5,:]=X[i,:]\n",
    "        target5[count5,:]=3\n",
    "        count5=count5+1\n",
    "    elif(y[i]=='V' and count12<numDataPerClass):#Ventricular\n",
    "        data12[count12,:]=X[i,:]\n",
    "        target12[count12,:]=4\n",
    "        count12=count12+1\n",
    "    i=i+1\n",
    "\n",
    "data=np.concatenate((data1,data2,data3,data5,data12),axis=0)\n",
    "target=np.concatenate((target1,target2,target3,target5,target12),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\t count1\t count12\t count2\t count3\t count5\t data\t data1\t data12\t \n",
      "data2\t data3\t data5\t dataColumns\t dataset\t i\t np\t np_utils\t numDataPerClass\t \n",
      "pd\t plt\t target\t target1\t target12\t target2\t target3\t target5\t testX\t \n",
      "testY\t trainX\t trainY\t train_test_split\t y\t \n"
     ]
    }
   ],
   "source": [
    "#View current variables in memory\n",
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete unneccessary variables\n",
    "del data1, data2, data3, data5, data12\n",
    "del target1, target2, target3, target5, target12\n",
    "del i, count1, count2, count3, count5, count12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable           Type         Data/Info\n",
      "-----------------------------------------\n",
      "X                  ndarray      29135x300: 8740500 elems, type `float64`, 69924000 bytes (66.68472290039062 Mb)\n",
      "data               ndarray      25000x300: 7500000 elems, type `float64`, 60000000 bytes (57.220458984375 Mb)\n",
      "dataColumns        int          301\n",
      "dataset            DataFrame                  0       0.1<...>29135 rows x 301 columns]\n",
      "np                 module       <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "np_utils           module       <module 'keras.utils.np_u<...>ras\\\\utils\\\\np_utils.py'>\n",
      "numDataPerClass    int          5000\n",
      "pd                 module       <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "plt                module       <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "target             ndarray      25000x1: 25000 elems, type `float64`, 200000 bytes (195.3125 kb)\n",
      "testX              ndarray      7500x300: 2250000 elems, type `float64`, 18000000 bytes (17.1661376953125 Mb)\n",
      "testY              ndarray      7500x5: 37500 elems, type `float32`, 150000 bytes (146.484375 kb)\n",
      "trainX             ndarray      17500x300: 5250000 elems, type `float64`, 42000000 bytes (40.0543212890625 Mb)\n",
      "trainY             ndarray      17500x5: 87500 elems, type `float32`, 350000 bytes (341.796875 kb)\n",
      "train_test_split   function     <function train_test_split at 0x0000019B8C58CF78>\n",
      "y                  ndarray      29135: 29135 elems, type `object`, 233080 bytes (227.6171875 kb)\n"
     ]
    }
   ],
   "source": [
    "#View current variables in memory (with some extra information)\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into 70(training):30(testing) ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainX,testX,trainY,testY=train_test_split(data,target,test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before one-hot encoding\n",
      "(17500, 1)\n",
      "(7500, 1)\n",
      "After one-hot encoding\n",
      "(17500, 5)\n",
      "(7500, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Before one-hot encoding')\n",
    "print(trainY.shape)\n",
    "print(testY.shape)\n",
    "\n",
    "#convert to one-hot encoding\n",
    "from keras.utils import np_utils\n",
    "\n",
    "trainY=np_utils.to_categorical(trainY)\n",
    "testY=np_utils.to_categorical(testY)\n",
    "print('After one-hot encoding')\n",
    "print(trainY.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First target vector in training data \t[1. 0. 0. 0. 0.]\n",
      "First target vector in testing data \t[0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('First target vector in training data \\t'+str(trainY[0]))\n",
    "print('First target vector in testing data \\t'+str(testY[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dhanjal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dhanjal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dhanjal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dhanjal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dhanjal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inport libraries necessary for keras models and layers\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "#Make Artificial neural network\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units = 152, kernel_initializer = 'uniform', activation = 'relu', input_dim = 300))\n",
    "classifier.add(Dense(units = 300, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 300, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 152, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 152)               45752     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               45900     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 152)               45752     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 765       \n",
      "=================================================================\n",
      "Total params: 228,469\n",
      "Trainable params: 228,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dhanjal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Dhanjal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/10\n",
      "17500/17500 [==============================] - 7s 423us/step - loss: 0.3078 - acc: 0.9001 - val_loss: 0.0794 - val_acc: 0.9751\n",
      "Epoch 2/10\n",
      "17500/17500 [==============================] - 2s 127us/step - loss: 0.0658 - acc: 0.9818 - val_loss: 0.0579 - val_acc: 0.9857\n",
      "Epoch 3/10\n",
      "17500/17500 [==============================] - 2s 131us/step - loss: 0.0412 - acc: 0.9881 - val_loss: 0.0448 - val_acc: 0.9876\n",
      "Epoch 4/10\n",
      "17500/17500 [==============================] - 2s 130us/step - loss: 0.0326 - acc: 0.9917 - val_loss: 0.0374 - val_acc: 0.9903\n",
      "Epoch 5/10\n",
      "17500/17500 [==============================] - 2s 128us/step - loss: 0.0318 - acc: 0.9912 - val_loss: 0.0359 - val_acc: 0.9896\n",
      "Epoch 6/10\n",
      "17500/17500 [==============================] - 2s 140us/step - loss: 0.0220 - acc: 0.9936 - val_loss: 0.0379 - val_acc: 0.9887\n",
      "Epoch 7/10\n",
      "17500/17500 [==============================] - 2s 125us/step - loss: 0.0202 - acc: 0.9942 - val_loss: 0.0410 - val_acc: 0.9884\n",
      "Epoch 8/10\n",
      "17500/17500 [==============================] - 2s 135us/step - loss: 0.0207 - acc: 0.9938 - val_loss: 0.0343 - val_acc: 0.9908\n",
      "Epoch 9/10\n",
      "17500/17500 [==============================] - 3s 148us/step - loss: 0.0155 - acc: 0.9955 - val_loss: 0.0264 - val_acc: 0.9932\n",
      "Epoch 10/10\n",
      "17500/17500 [==============================] - 2s 134us/step - loss: 0.0115 - acc: 0.9958 - val_loss: 0.0484 - val_acc: 0.9896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19b9ae4af08>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training a classifier i.e. making it learn to differentiate between 1 normal and 4 abnormal classes\n",
    "classifier.fit(trainX, trainY, batch_size = 100, epochs = 10, verbose=1,\n",
    "          validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save trained classifier for future\n",
    "classifier.save('classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04840311065988305\n",
      "Test accuracy: 0.9896000000317892\n"
     ]
    }
   ],
   "source": [
    "score = classifier.evaluate(testX, testY, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[1492    0    0    0    3]\n",
      " [   0 1503    0    3    0]\n",
      " [   0    0 1453    1    4]\n",
      " [   0    1    1 1533    2]\n",
      " [   4   30   10   19 1441]]\n",
      "Normalized confusion matrix\n",
      "[[9.98e-01 0.00e+00 0.00e+00 0.00e+00 2.01e-03]\n",
      " [0.00e+00 9.98e-01 0.00e+00 1.99e-03 0.00e+00]\n",
      " [0.00e+00 0.00e+00 9.97e-01 6.86e-04 2.74e-03]\n",
      " [0.00e+00 6.51e-04 6.51e-04 9.97e-01 1.30e-03]\n",
      " [2.66e-03 1.99e-02 6.65e-03 1.26e-02 9.58e-01]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1495\n",
      "           1       0.98      1.00      0.99      1506\n",
      "           2       0.99      1.00      0.99      1458\n",
      "           3       0.99      1.00      0.99      1537\n",
      "           4       0.99      0.96      0.98      1504\n",
      "\n",
      "    accuracy                           0.99      7500\n",
      "   macro avg       0.99      0.99      0.99      7500\n",
      "weighted avg       0.99      0.99      0.99      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "predY=classifier.predict(testX)\n",
    "predY=np.argmax(predY,axis=1)\n",
    "testY1=np.argmax(testY,axis=1)\n",
    "\n",
    "                        #Compute Confusion Matrix\n",
    "cm=confusion_matrix(testY1,predY)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "                        #Compute Normalized Confusion Matrix\n",
    "cm_normalized=cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "\n",
    "                        #ANN  Classification report\n",
    "ann_classification_report=classification_report(testY1,predY)\n",
    "print(ann_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 300, 1)\n",
      "(7500, 300, 1)\n"
     ]
    }
   ],
   "source": [
    "#Need to change shape of data (keras conventional input) for convolutional neural network\n",
    "trainX=np.reshape(trainX,(-1,300,1))\n",
    "testX=np.reshape(testX,(-1,300,1))\n",
    "print(trainX.shape)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a simple CNN (other way)\n",
    "input_tensor=Input((300,1))\n",
    "\n",
    "#Adding convolutional layers\n",
    "conv=Conv1D(filters=64,kernel_size=3, strides=1, padding='valid')(input_tensor)\n",
    "actv=Activation(\"relu\")(conv)\n",
    "maxP=MaxPooling1D(pool_size=2)(actv)\n",
    "\n",
    "conv=Conv1D(filters=32,kernel_size=5, strides=2, padding='same')(maxP)\n",
    "actv=Activation(keras.activations.relu)(conv)\n",
    "maxP=MaxPooling1D(pool_size=2)(actv)\n",
    "\n",
    "#flattening the nd-array to 1d-array\n",
    "f_vector=Flatten()(maxP)\n",
    "\n",
    "#Add Dense layers- Artificial Neural Network\n",
    "dense=Dense(units=500)(f_vector)\n",
    "actv=Activation(keras.activations.relu)(dense)\n",
    "dense=Dense(units=100)(actv)\n",
    "actv=Activation(keras.activations.relu)(dense)\n",
    "dense=Dense(units=5)(actv)\n",
    "out=Activation(keras.activations.softmax)(dense)\n",
    "\n",
    "#Combine all layers in Model Object\n",
    "cnn_classifier=Model(inputs=input_tensor,outputs=out)\n",
    "cnn_classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 298, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 298, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 149, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 75, 32)            10272     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 75, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 37, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1184)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               592500    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 505       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 653,633\n",
      "Trainable params: 653,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/10\n",
      "17500/17500 [==============================] - 10s 572us/step - loss: 0.1871 - acc: 0.9458 - val_loss: 0.0525 - val_acc: 0.9856\n",
      "Epoch 2/10\n",
      "17500/17500 [==============================] - 7s 414us/step - loss: 0.0406 - acc: 0.9893 - val_loss: 0.0455 - val_acc: 0.9844\n",
      "Epoch 3/10\n",
      "17500/17500 [==============================] - 7s 423us/step - loss: 0.0223 - acc: 0.9938 - val_loss: 0.0181 - val_acc: 0.9943\n",
      "Epoch 4/10\n",
      "17500/17500 [==============================] - 7s 416us/step - loss: 0.0135 - acc: 0.9957 - val_loss: 0.0176 - val_acc: 0.9933\n",
      "Epoch 5/10\n",
      "17500/17500 [==============================] - 7s 418us/step - loss: 0.0097 - acc: 0.9971 - val_loss: 0.0158 - val_acc: 0.9956\n",
      "Epoch 6/10\n",
      "17500/17500 [==============================] - 8s 433us/step - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0144 - val_acc: 0.9955\n",
      "Epoch 7/10\n",
      "17500/17500 [==============================] - 7s 424us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0194 - val_acc: 0.9941\n",
      "Epoch 8/10\n",
      "17500/17500 [==============================] - 8s 431us/step - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0101 - val_acc: 0.9964\n",
      "Epoch 9/10\n",
      "17500/17500 [==============================] - 7s 416us/step - loss: 0.0023 - acc: 0.9991 - val_loss: 0.0138 - val_acc: 0.9952\n",
      "Epoch 10/10\n",
      "17500/17500 [==============================] - 7s 413us/step - loss: 0.0042 - acc: 0.9985 - val_loss: 0.0171 - val_acc: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19bc2202608>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training a classifier i.e. making it learn to differentiate between 1 normal and 4 abnormal classes\n",
    "cnn_classifier.fit(trainX, trainY, batch_size = 100, epochs = 10, verbose=1,\n",
    "          validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save classifier\n",
    "cnn_classifier.save('cnn_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[1493    0    1    0    1]\n",
      " [   0 1504    0    2    0]\n",
      " [   0    0 1455    0    3]\n",
      " [   0    0    0 1534    3]\n",
      " [   2    3    3   16 1480]]\n",
      "Normalized confusion matrix\n",
      "[[9.99e-01 0.00e+00 6.69e-04 0.00e+00 6.69e-04]\n",
      " [0.00e+00 9.99e-01 0.00e+00 1.33e-03 0.00e+00]\n",
      " [0.00e+00 0.00e+00 9.98e-01 0.00e+00 2.06e-03]\n",
      " [0.00e+00 0.00e+00 0.00e+00 9.98e-01 1.95e-03]\n",
      " [1.33e-03 1.99e-03 1.99e-03 1.06e-02 9.84e-01]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1495\n",
      "           1       1.00      1.00      1.00      1506\n",
      "           2       1.00      1.00      1.00      1458\n",
      "           3       0.99      1.00      0.99      1537\n",
      "           4       1.00      0.98      0.99      1504\n",
      "\n",
      "    accuracy                           1.00      7500\n",
      "   macro avg       1.00      1.00      1.00      7500\n",
      "weighted avg       1.00      1.00      1.00      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "predY=cnn_classifier.predict(testX)\n",
    "predY=np.argmax(predY,axis=1)\n",
    "testY1=np.argmax(testY,axis=1)\n",
    "\n",
    "                        #Compute Confusion Matrix\n",
    "cm=confusion_matrix(testY1,predY)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "                        #Compute Normalized Confusion Matrix\n",
    "cm_normalized=cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "\n",
    "                        #ANN  Classification report\n",
    "ann_classification_report=classification_report(testY1,predY)\n",
    "print(ann_classification_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
